---
title: "Introduction to Deepl-Learning"
format: html
---

The worlds most valuable recource is no longer oul, but data.
*But*: (Big) Data has no value, per se.  The crucial question is what can be learned from the data and what conclusions can be drawn.
-> Two main tasks are prediction and causal inference 

Object Detection Repository: https://github.com/matterport/Mask_RCNN
Wäre funny mal mit rum zu spielen.

Deeplearning ist ein bereich von Machine Learning und dieser wiederum ein Bereich von Artificial Intelligence (AI).

Klassisches Programmieren nutzt Regeln und Daten um eine Funktion (Antwort) zu erstellen, die ein Problem löst.

Machine Learning nutzt Daten und Antworten um eine Funktion zu erstellen, die ein Problem löst.

Wann sollte man Deeplearning nutzen und warum ist es so populär?
    Traditionelle Machine Learning Modelle haben eine begrenzte Performance, um Daten zu verarbeiten. Dies führt zu einem Plateau in der Performance. Deeplearning Modelle haben eine höhere Performance die mit der Menge an daten zunimmt.
    Generell ist zu sagen das die meisten Deeplearning Modelle andere Machine Learning Modelle übertreffen, wenn genug Daten vorhanden sind.

Expressions in Deeplearning
    - Training/Learning/Estimating
    - Weights/Parameters
    - Outcome/Label/ Dependent Variable
    - Features/Independent Variables/ Input/ Regressor

Software-frameworks
    Es gibt verschiedene Frameworks, in dieser Vorlesung wird PyTorch verwendet. Es gibt aber auch Tensorflow, Keras, Theano, Caffe, CNTK, MXNet, Chainer, Deeplearning4j, Gluon, Caffe2, PaddlePaddle, BigDL, DL4J, Neon, CaffeOnSpark. 

Tensors und Linear Algebra
    Deeplearning ist auf Große Datensätze angewiesen und diese können in Form von Tensoren dargestellt werden. Tensoren sind eine Verallgemeinerung von Matrizen und Vektoren. Deshalb ist die Verwendung von Basic Linear Algebra hilfreich und nutzlich.
    In Pytorch gibt es die Klasse Tensor, die die Basis für alle Berechnungen in Pytorch ist. Ein Tensor is eine n-dimensional array von nummerischen Werten. Die Tensor-Klasse ist vergleich mit der Numpy-Array-Klasse, aber mit zusätzlichen Funktionen für GPU-Berechnungen und automatisches Differenzieren.

Create Tensors
    Um einen Tensor zu erstellen, startet man mit dem importieren von torchmodul und erstellt einen simplen tensor.

```{python}
import torch
x = torch.arange(42)
print(x)
```

Als nächstes die eigenschaften des Tensos ausgeben.

```{python} 
print(x.shape)
```

Wenn man die Form neu definieren möchte, kann Numpy hilfreich sein.

```{python} 
X = x.reshape(6,7) #6 Zeilen und 7 Spalten
print(X)
print(X.shape)
```

Die Form eines Tensors kann auch mit -1 definiert werden, um die Form automatisch zu berechnen.

```{python}
X = x.reshape(6,-1) #6 Zeilen und 7 Spalten
print(X)
print(X.shape)
```

```{python}
X = x.reshape(-1,7) #6 Zeilen und 7 Spalten
print(X)
print(X.shape)
```

Anstatt reshape kann auch ein Vektor in einem bestimmten Format erstellt werden.

```{python}
print (torch.tensor([[1,2],[3,4],[5,6]]))
```

Allgemein werden hier die tensoren mit randomisierten Werten, einsen oder nullen bei spezifischen Dimensionen erstellt.

```{python}
zeros = torch.zeros((2,3,4)) # 2 Matrizen mit 3 Zeilen und 4 Spalten
print(zeros)
```

```{python}
ones = torch.ones((2,3,4)) # 2 Matrizen mit 3 Zeilen und 4 Spalten
print(ones)
```

```{python}
random_tensor = torch.randn(2,3,4) # 2 Matrizen mit 3 Zeilen und 4 Spalten
print(random_tensor)
````

Hier erstellt torch.randn einen Tensor mit zufälligen Werten, die einer Normalverteilung folgen.

Tensor Operations
    Tensoren können mit den üblichen Operationen wie Addition, Subtraktion, Multiplikation und Division bearbeitet werden. Diese Operationen können auch auf Tensoren mit unterschiedlichen Formen angewendet werden, wenn die Bedingungen für die Broadcasting-Regeln erfüllt sind. Broadcasting-Regeln sind Regeln, die die Bedingungen für die Anwendung von Operationen auf Tensoren mit unterschiedlichen Formen definieren. Zum Beispiel können zwei Tensoren mit unterschiedlichen Formen addiert werden, wenn die Formen der Tensoren einer oder beiden Dimensionen gleich sind.

```{python}
x = torch.tensor([3.0,2])
y = torch.tensor([4.0,1])
print(x+y, x-y, x*y, x/y, x**2)
```

Außerdem können bekannte funktionen wie exponential oder lograithm angewendet werden.

```{python}
a = torch.exp(x)
print(a, torch.log(a))
```

Außerdem können Tensoren in sehr wichtigen Operationen, welche in concatenate operation (torch.cat) und dot product (torch.mm) bestehen, verwendet werden.

```{python}
cat_0 = torch.cat((zeros,ones), dim=0) # dim=0 bedeutet, dass die Tensoren in der ersten Dimension (Zeilen) zusammengefügt werden
print(cat_0)
```

```{python}
cat_1 = torch.cat((zeros, ones), dim=1) # dim=1 bedeutet, dass die Tensoren in der zweiten Dimension (Spalten) zusammengefügt werden
print(cat_1)
```

```{python}
print(torch.cat((zeros, ones), dim=2)) # dim=2 bedeutet, dass die Tensoren in der dritten Dimension zusammengefügt werden
```

Andere wichtige Operationen sind die Transposition und der Dot-Product zweier Tensoren. Transposition bedeuetet, dass die Zeilen und Spalten eines Tensors vertauscht werden. Der Dot-Product zweier Tensoren ist das Produkt der Elemente der beiden Tensoren. Das Produkt wird berechnet, indem die Elemente der ersten Zeile der ersten Matrix mit den Elementen der ersten Spalte der zweiten Matrix multipliziert werden. Die Ergebnisse werden dann addiert, um das erste Element der Ergebnismatrix zu erhalten.
Der Dot-Product zweier Tensoren kann mit der Funktion torch.mm berechnet werden. 


# Einführung in Machine Learning

```{python}
print(x,y,x < y) 
```
```{python}
print(x.sum(),x.mean())
```

Das Konzept of Broadcasting ist nützlich um kalkulationen zu vereinbaren. Boradcasting beschreibt die Regeln, die die Bedingungen für die Anwendung von Operationen auf Tensoren mit unterschiedlichen Formen definieren. Zum Beispiel können zwei Tensoren mit unterschiedlichen Formen addiert werden, wenn die Formen der Tensoren einer oder beiden Dimensionen gleich sind.

```{python}
x = torch.tensor([[3.0], [2]])
y = torch.tensor([[4.0,1,2]])
print(x.shape, y.shape)
````

Torch.tensor bewirkt, dass die Tensoren die gleiche Form haben. Die Tensoren können dann addiert werden.

```{python}
z = x+y
print(z, z.shape)
```

Der Output zeigt das der Tensor x (dimension 2x1) und y (1x3) werden broadcasted zu zwei einem Tensor z (2x3). Hierbei werden die Tensoren dem Shape des anderen Tensors angepasst. In dem der x Tensor in der zweiten Dimension (Spalten) verdoppelt wird und der y Tensor in der ersten Dimension (Zeilen) verdoppelt wird.

$x+y = \begin{bmatrix} 3 & 3 & 3 \\ 2 & 2 & 2 \end{bmatrix} + \begin{bmatrix} 4 & 1 & 2 \\ 4 & 1 & 2 \end{bmatrix} = \begin{bmatrix} 7 & 4 & 5 \\ 6 & 3 & 4 \end{bmatrix} $

Außerdem können spezifische Elemente mittels indexing und sclicing ausgewählt werden.

```{python}
a = torch.arange(12).reshape(-1,4)
print(a)
```

```{python}
print(a[0,0], a[0,:], a[0:2,0])
```

```{python}
print(a[:,3], a[:,-1])
```

Wie viele andere Numpy Funktionen ist das konvertieren von array zu tensoren sehr hilfreich.

```{python}
b = a.numpy()
print(b)
```

```{python}
print(torch.tensor(b))
```

## Linear Algebra

Da viele Tensoren Matrixen oder Vektoren sein werden, ist es hilfreich anwendungen der Linearen Algebra zu kennen. Ein Vektor ist eine Matrix mit einer Spalte und eine Matrix ist ein Tensor mit zwei Dimensionen. Ein Tensor mit drei Dimensionen ist ein Würfel.

Um das Skalarprodukt (dot product) zu berechnen gilt die Formel:

$x=\begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_p \end{bmatrix}$ und $y=\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_p \end{bmatrix}$

dann ist
$(x,y) =x^t y = \sum_{i=1}^{p} x_i y_i$

was in torch.dot berechnet werden kann.

```{python}
x = torch.arange(3, dtype = torch.float32)
y = torch.arange(3, dtype = torch.float32)
print(x, y, torch.dot(x,y))
```

Um 
# Diggah wie geht die Formel?

```{python}
A = torch.tensor([[3,1.3,0],[2,5,0.5]])
print(A.shape, x.shape)
```

```{python}
print(torch.mv(A,x))
````

```{python}
B = torch.ones((3,2))
print(torch.mm(A,B))
```

Vektor-Norm bedeutet das die Länge eines Vektors berechnet wird. Die Länge eines Vektors ist die Wurzel der Summe der Quadrate der Elemente des Vektors. Die Norm eines Vektors x ist definiert als ||x|| = sqrt(x^t x). Die Norm eines Vektors kann mit der Funktion torch.norm berechnet werden.

```{python}
print(x,torch.norm(x))
```

Die Euclidean Norm ist ein besonderer Fall der L_p-norm

hier ist der L_1-norm ein wichtiger Fall. Wie unterscheidet sich das verhalten von L_1 und L_2 Normen?
Eine Matrix-norm hat die selben Eigenschaften wie eine Vektor-norm, aber ist definiert ür Matrizen (generell werden normen definiert als Vektor räume). 

Die Frobenius norm ist eine Matrix-norm, die die Länge einer Matrix berechnet. Die Frobenius norm einer Matrix A ist definiert als ||A||_F = sqrt(sum_i sum_j a_ij^2). 

```{python}
print(A, torch.norm(A))
```

Die Frobenius Norm ist ein spezialfall der L_P,q-norm.

## Basics of Machine Learning 

Machine Learning kann als eine Funktionsapproximation gesehen werden. Mit X als den Input und Y als den Output, wird eine Funktion f(X) gesucht, die Y approximiert. Die Funktion f(X) wird durch die Anpassung der Parameter der Funktion an die Daten gelernt. Die Daten werden in Trainings- und Testdaten unterteilt. Die Trainingsdaten werden verwendet, um die Funktion zu lernen und die Testdaten werden verwendet, um die Leistung der Funktion zu bewerten.

Wie X,Y und f_0 aussieht hängt stark davon ab welches learning problem vorliegt. Es gibt zwei Hauptkategorien von learning problems: Supervised und Unsupervised Learning.

Supervised Learning
    In supervised learning wird ein Modell erstellt, das eine Funktion f(X) lernt, die die Beziehung zwischen den Input X und den Output Y approximiert (eine annäherung an den tatsächlichen Wert). Die Trainingsdaten bestehen aus Input-Output-Paaren (X,Y). Das Ziel ist es, eine Funktion zu finden, die die Trainingsdaten gut approximiert und die Testdaten gut generalisiert. Es gibt zwei Hauptkategorien von supervised learning: Regression und Classification.
    Zum Beispiel
    - Image Classification: Ein Bild wird als Input gegeben und das Modell muss das Bild in eine von mehreren Klassen klassifizieren.
    - Sales Prediction: Historische Verkaufsdaten werden als Input gegeben und das Modell muss die zukünftigen Verkaufszahlen vorhersagen.
    - Translation: Ein Satz in einer Sprache wird als Input gegeben und das Modell muss den Satz in eine andere Sprache übersetzen.

Unsupervised Learning
    Im Kontrast zu supervised learning gibt es keine Output-Labels in unsupervised learning. Das Ziel ist es, Muster in den Daten zu finden. Es gibt zwei Hauptkategorien von unsupervised learning: 
    - Clustering und 
    - Dimensionality Reduction.

Semi-Supervised Learning
    Semi-supervised learning ist eine Kombination aus supervised und unsupervised learning. Es wird verwendet, wenn nur ein Teil der Daten gelabelt ist. Das Modell lernt aus den gelabelten und ungelaubelten Daten.

Regression vs. Classification
    Wenn die Target-Variable kontinuierlich ist, wird das Problem als Regression bezeichnet. Wenn die Target-Variable kategorisch ist, wird das Problem als Classification bezeichnet.

Loss Function
    Die Loss Function ist eine Funktion, die die Differenz zwischen den tatsächlichen und den vorhergesagten Werten misst. Das Ziel ist es, die Loss Function zu minimieren. Die Loss Function hängt von der Art des Problems ab. Zum Beispiel ist die Mean Squared Error (MSE) eine gängige Loss Function für Regression und die Cross-Entropy Loss Function eine gängige Loss Function für Classification.