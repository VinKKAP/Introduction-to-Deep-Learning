{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "With the rise of digitalization the availability of data has changed drastically:\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/BigData_001.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/devices.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr>\n",
    "</table><br>\n",
    "<center><b>Figure 1:</b> Data collection now and then.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data becomes more and more important.\n",
    "\n",
    "<center><i>The world's most valuable resource is no longer oil, but data.</i></center>\n",
    "<center><img src=\"figures/Introduction_DL/oil.png\" alt=\"Drawing\" style=\"width: 400px;\"/><br>\n",
    "<b>Figure 2:</b> From: The Economist (May 6th, 2017).</center>\n",
    "\n",
    "**But:** (Big) Data has no value, per se.<br> The crucial question is what can be learned from the data and what conclusions can be drawn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\\Rightarrow$ Two main tasks are **prediction** and **causal inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Breakthroughs and Applications of Machine Learning\n",
    "<table><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/alphago.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/nature.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table><br>\n",
    "<center><b>Figure 3:</b> Alpha Go.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"figures/Introduction_DL/ml-translation.png\" alt=\"Drawing\" style=\"width: 800px;\"/><br>\n",
    "<b>Figure 4:</b> Google Translate. From: https://uk.pcmag.com/internet-3/86069/google-expands-neural-networks-for-language-translation.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/chart_top1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/chart_top5.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table><br>\n",
    "<center><b>Figure 5:</b> Change in Performance for Image Classification. From: https://paperswithcode.com/sota/image-classification-on-imagenet.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"figures/Introduction_DL/street.png\" alt=\"Drawing\" style=\"width: 600px;\"/><br>\n",
    "<b>Figure 6:</b> Object Detection and Segmentation. From: https://github.com/matterport/Mask_RCNN.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table cellspacing=\"0\" cellpadding=\"0\"><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/person_1.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_2.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_3.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_4.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/person_5.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_6.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_7.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/person_8.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table><br>\n",
    "<center><b>Figure 7:</b> Generate Faces. From: https://thispersondoesnotexist.com/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"figures/Introduction_DL/DL_art_pic2.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/><br>\n",
    "<b>Figure 8:</b> \"Edmond de Belamy\". From: Christie's.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table cellspacing=\"0\" cellpadding=\"0\"><tr>\n",
    "<td></td>\n",
    "<td> <img src=\"figures/Introduction_DL/venice-boat.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td></td>\n",
    "</tr><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/g2.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/g7.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/g9.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr><tr>\n",
    "<td> <img src=\"figures/Introduction_DL/style_1.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/style_2.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figures/Introduction_DL/style_3.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table><br>\n",
    "<center><b>Figure 9:</b> Neural Style transfer. From: https://github.com/StacyYang/MXNet-Gluon-Style-Transfer</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples in Different Domains\n",
    "\n",
    "- Near-human-level image classification\n",
    "- Near-human-level speech recognition\n",
    "- Near-human-level handwriting transcription\n",
    "- Improved machine translation\n",
    "- Improved text-to-speech conversion\n",
    "- Near-human-level autonomous driving\n",
    "- Improved search results on the web\n",
    "- Ability to answer natural language questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Intelligence, Machine Learning and Deep Learning\n",
    "\n",
    "There are a lot of terms, which are often used when talking about deep learning.<br> \n",
    "Let us clarify the relationship between artificial intelligence, machine learning and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"figures/Introduction_DL/AI_ML_DL.png\" alt=\"Drawing\" style=\"width: 400px\"/>\n",
    "<br></center>\n",
    "<center><b>Figure 10:</b> Relationship between AI, machine learning and deep learning.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Machine Learning Paradigm\n",
    "Classical programming takes data and explicit, designed rules as input to produce an output.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/Introduction_DL/intro_ml_paradigm_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/><br>\n",
    "<b>Figure 11:</b> Classical programming.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead machine learning \"learns\" to design rules from data and the corresponding answers.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/Introduction_DL/intro_ml_paradigm_2.png\" style=\"width: 800px;\"/><br>\n",
    "<b>Figure 12:</b> Machine learning.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to use Deep Learning\n",
    "\n",
    "As there are a lot of different machine learning algorithms (eg. boosting, random forests,...) the question is when to use deep learning or why deep learning has become so popular over the last years.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/Introduction_DL/DLvsML.png\"  style=\"width: 400px;\"/><br></center>\n",
    "<center><b>Figure 13:</b> Comparison of deep learning and traditional machine learning algorithms.</center>\n",
    "\n",
    "In general, deep learning will outperform most of the other machine learning methods, if the amount of data is very huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Expressions \n",
    "\n",
    "As the deep learning community has some unique expressions, which might be called differently in statistic literature, we provide a list of synonymous expressions\n",
    "\n",
    "- Training/Learning/Estimating\n",
    "- Weights/Parameters\n",
    "- Outcome/Label/Dependent Variable\n",
    "- Feature/Input/Regressor/Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "There are several different deep learning software-frameworks (cf. [Deep Learning Software](https://en.wikipedia.org/wiki/Comparison_of_deep-learning_software))\n",
    "\n",
    "In this lecture, we will use [Pytorch](https://pytorch.org/). Pytorch is an open source machine learning framework, which is also very popular in companies (another large platform is [Tensorflow](https://www.tensorflow.org/)).\n",
    "Pytorch is based on Python, such that you will require some additional prerequisites (like Python etc.). As you will write your own code in the tutorials, you have to get access to Pytorch.\n",
    "\n",
    "You can either use a [local installation](https://pytorch.org/get-started/locally/) (using Anaconda can be really helpful) or use some online solutions as [Colab](https://colab.research.google.com/notebooks/intro.ipynb) or \n",
    "[Kaggle](https://www.kaggle.com/docs/notebooks) (which require registrations, but might be very easy to set up, since you do not need to install packages).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors and Linear Algebra\n",
    "\n",
    "Before we start to learn any concepts of machine learning, we will start be introducing some basic mathematics and the corresponding implementation in Pytorch.\n",
    "\n",
    "Deep learning heavily relies on working with large datasets, which can be represented as matrices, where the rows represent the observations and the columns the corresponding attributes or features. Therefore, basic operations from linear algebra are highly useful.\n",
    "\n",
    "<center><img src=\"figures/Introduction_DL/matrix_small.png\" alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Pytorch the data is stored in the `tensor class`. A tensor is the $n$-dimensional array of numerical values. Therefore, tensors are a generalization of a matrix, such as matrices are the two-dimensional generalization of vectors. \n",
    "\n",
    "The `tensor class` from Pytorch is similar to `ndarray class` from NumPy, but includes some important additional features, such as GPU-support and automatic differentiation support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Tensors\n",
    "To create a tensor we start by importing the torch module and creating some basic tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(42)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we list some of the most basic forms of properties and operations on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we want to reshape the data, the reshape command (as in NumPy) is very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12, 13],\n",
      "        [14, 15, 16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25, 26, 27],\n",
      "        [28, 29, 30, 31, 32, 33, 34],\n",
      "        [35, 36, 37, 38, 39, 40, 41]])\n",
      "torch.Size([6, 7])\n"
     ]
    }
   ],
   "source": [
    "X = x.reshape(6,7)\n",
    "print(X) \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is the same as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12, 13],\n",
      "        [14, 15, 16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25, 26, 27],\n",
      "        [28, 29, 30, 31, 32, 33, 34],\n",
      "        [35, 36, 37, 38, 39, 40, 41]])\n",
      "torch.Size([6, 7])\n"
     ]
    }
   ],
   "source": [
    "X = x.reshape(6,-1)\n",
    "print(X) \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12, 13],\n",
      "        [14, 15, 16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25, 26, 27],\n",
      "        [28, 29, 30, 31, 32, 33, 34],\n",
      "        [35, 36, 37, 38, 39, 40, 41]])\n",
      "torch.Size([6, 7])\n"
     ]
    }
   ],
   "source": [
    "X = x.reshape(-1,7)\n",
    "print(X) \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of reshaping a vector to specific shape, we can specify the tensor directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([[1,2],[3,4],[5,6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, we initialize tensors with random values, ones or zeros by specifying the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2,3,4)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2,3,4)\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1431e+00,  1.7184e+00,  2.0493e-02, -3.0263e-01],\n",
      "         [ 8.9599e-01, -1.3769e+00, -1.0565e+00, -1.1656e+00],\n",
      "         [-1.2166e+00, -2.0612e+00,  1.4955e+00, -7.4260e-02]],\n",
      "\n",
      "        [[-1.3707e+00,  2.0854e+00,  6.5237e-01, -9.0319e-01],\n",
      "         [ 4.1972e-01, -1.5094e-03,  2.8426e-01, -3.4373e-01],\n",
      "         [-2.0581e+00, -1.5252e-01,  7.8396e-01, -6.8375e-02]]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(2,3,4)\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here, `torch.randn` creates randomly drawn entries, where each entry is drawn independently from a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor Operations\n",
    "\n",
    "#### Basic Operations\n",
    "\n",
    "The most common arithmetic operators (addition, subtraction, multiplication, division and  exponentiation) are implemented to work elementwise on the tensor class for two tensors of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7., 3.]) tensor([-1.,  1.]) tensor([12.,  2.]) tensor([0.7500, 2.0000]) tensor([9., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0,2])\n",
    "y = torch.tensor([4.0,1])\n",
    "print(x+y, x-y, x*y, x/y, x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Additionally, some common functions as the exponential or logarithmic function are implemented and executed elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.0855,  7.3891]) tensor([3., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.exp(x)\n",
    "print(a, torch.log(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further, combining tensors is a very important operation, which can be performed with the concatenate operation (`torch.cat`). To use `torch.cat`, two compatible tensors and the axis to concatenate along have to specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "cat_0 = torch.cat((zeros,ones), dim = 0)\n",
    "print(cat_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]]) tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "cat_1 = torch.cat((zeros,ones), dim = 1)\n",
    "print(cat_0, cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((zeros,ones), dim = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, logical statements and some other basic operations can be easily used for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 2.]) tensor([4., 1.]) tensor([ True, False])\n"
     ]
    }
   ],
   "source": [
    "print(x,y,x < y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "print(x.sum(),x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The concept of *Broadcasting* is very useful to simplify some calculations.<br> As seen above the elementwise operations, need the tensors to be of the same shape. \n",
    "\n",
    "Under certain conditions, the elementwise operations can even be applied to tensors of different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3.0],[2]])\n",
    "y = torch.tensor([[4.0,1,2]])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 4., 5.],\n",
      "        [6., 3., 4.]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "z = x+y\n",
    "print(z,z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can observe, the tensor $x$ (dimension $2\\times 1$) and $y$ (dimension $1\\times 3$) are broadcasted to the same shape ($2\\times 3$) with an subsequent elementwise operation:\n",
    "$$x+y = \\begin{pmatrix} 3 & 3 &3 \\\\ 2 & 2 & 2\\end{pmatrix}+\\begin{pmatrix} 4 & 1 &2 \\\\ 4 & 1 & 2\\end{pmatrix}= \\begin{pmatrix}7 & 4 &5 \\\\ 6 & 3 & 4\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further, specific elements can be accessed with indexing and slicing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(-1,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor([0, 1, 2, 3]) tensor([0, 4])\n"
     ]
    }
   ],
   "source": [
    "print(a[0,0], a[0,:], a[0:2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  7, 11]) tensor([ 3,  7, 11])\n"
     ]
    }
   ],
   "source": [
    "print(a[:,3], a[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As many some functions need NumPy arrays converting converting tensors is very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linear Algebra\n",
    "\n",
    "Recall that a lot of tensors will be matrices or vectors. Therefore, operations from linear algebra will be very helpful. At first we will take a look at dot products.\n",
    "Let \n",
    "\n",
    "$$x = \\begin{pmatrix}x_1\\\\\n",
    "x_2\\\\ \\vdots\\\\x_p \\end{pmatrix}\\in \\mathbb{R}^p,y= \\begin{pmatrix}y_1\\\\\n",
    "y_2\\\\ \\vdots\\\\y_p \\end{pmatrix}\\in \\mathbb{R}^p,$$ \n",
    "\n",
    "then \n",
    "\n",
    "$$\\langle x, y\\rangle = x^Ty := \\sum_{i=1}^p x_iy_i, $$\n",
    "\n",
    "which is implemented with `torch.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.]) tensor([1., 1., 1.]) tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3, dtype = torch.float32)\n",
    "y = torch.ones(3, dtype = torch.float32)\n",
    "print(x, y, torch.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let \n",
    "\n",
    "$$A=\\begin{pmatrix}a_{1,1}&a_{1,2}&\\cdots& a_{1,p}\\\\\n",
    "a_{2,1}&a_{2,2}&\\cdots& a_{2,p}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "a_{n,1}&a_{n,2}&\\cdots&a_{n,p}\\end{pmatrix}\\in\\mathbb{R}^{n\\times p}$$ \n",
    "\n",
    "\n",
    "and $x\\in \\mathbb{R}^p$ then the matrix-vector product is defined as\n",
    "\n",
    "$$Ax:=\\begin{pmatrix}\\sum_{j=1}^n a_{1,j} x_j\\\\\n",
    "\\vdots\\\\\n",
    "\\sum_{j=1}^n a_{n,j} x_j\\end{pmatrix}=\\begin{pmatrix} a_{1,\\bullet}^T x_j\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,\\bullet}^T x_j\\end{pmatrix}\\in \\mathbb{R}^n,$$\n",
    "\n",
    "where $a_{i,\\bullet}^T$ denotes the $i$-th row of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use `torch.mv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[3,1.3,0],[2,5,0.5]])\n",
    "print(A.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3000, 6.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mv(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $B\\in \\mathbb{R}^{p\\times q}$. Then the generalization to matrix-matrix product is straightforward.\n",
    "\n",
    "$$AB=\\begin{pmatrix}a_{1,\\bullet}^T b_{\\bullet,1}&a_{1,\\bullet}^T b_{\\bullet,2}&\\cdots& a_{1,\\bullet}^T b_{\\bullet,q}\\\\\n",
    "a_{2,\\bullet}^T b_{\\bullet,1}&a_{2,\\bullet}^T b_{\\bullet,2}&\\cdots& a_{2,\\bullet}^T b_{\\bullet,q}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "_{n,\\bullet}^T b_{\\bullet,n}&a_{1,\\bullet}^T b_{\\bullet,2}&\\cdots& a_{n,\\bullet}^T b_{\\bullet,q}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "where $b_{\\bullet,j}$ denotes the $j$-th column of $B$. This can be done with `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3000, 4.3000],\n",
      "        [7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.ones((3,2))\n",
    "print(torch.mm(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $x\\in\\mathbb{R}^n$ be vector. A **vector-norm** is a function $f:\\mathbb{R}^n\\to [0,\\infty)$ with the following properties\n",
    "\n",
    "1. For $\\alpha\\in \\mathbb{R}$ it holds: $f(\\alpha x)=|\\alpha|f(x)$.\n",
    "2. For $x,y\\in\\mathbb{R}^n$ it holds: $f(x+y)\\le f(x)+f(y)$.\n",
    "3. It holds: $f(x)=0 \\Leftrightarrow x=0\\in \\mathbb{R}^n$.\n",
    "\n",
    "If only properties 1. and 2. hold, $f$ is called a seminorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most common norm is the euclidean norm \n",
    "\n",
    "$$\\begin{aligned}\\|\\cdot\\|_2:&\\mathbb{R}^n\\to [0,\\infty)\\\\ \n",
    "&x\\mapsto \\left(\\sum\\limits_{i=1}^n x_i^2\\right)^{1/2}\n",
    "\\end{aligned}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.]) tensor(2.2361)\n"
     ]
    }
   ],
   "source": [
    "print(x,torch.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The euclidean norm is a special case of the $L_p$-norm \n",
    "\n",
    "$$\\begin{aligned}\\|\\cdot\\|_p:&\\mathbb{R}^n\\to [0,\\infty)\\\\ \n",
    "&x\\mapsto \\left(\\sum\\limits_{i=1}^n x_i^p\\right)^{1/p}\n",
    "\\end{aligned},$$\n",
    "\n",
    "where the $L_1$-norm is an important case. How does the $L_1$-norm behave differently than the $L_2$-norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **matrix-norm** has the same properties as a vector norm, but is defined for matrices (in general norms are defined on vector spaces).\n",
    "\n",
    "Let $A\\in \\mathbb{R}^{n\\times m}$. The **Frobenius norm** \n",
    "\n",
    "$$\\begin{aligned}\\|\\cdot\\|_F:&\\mathbb{R}^{n\\times m}\\to [0,\\infty)\\\\ \n",
    "&A\\mapsto \\left(\\sum\\limits_{i=1}^n\\sum_{j=1}^m a_{i,j}^2\\right)^{1/2}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0000, 1.3000, 0.0000],\n",
      "        [2.0000, 5.0000, 0.5000]]) tensor(6.3198)\n"
     ]
    }
   ],
   "source": [
    "print(A,torch.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Frobenius norm is a special case of the $L_{p,q}$-norm \n",
    "\n",
    "$$\\begin{aligned}\\|\\cdot\\|_{p,q}:&\\mathbb{R}^{n\\times m}\\to [0,\\infty)\\\\ \n",
    "&A\\mapsto \\left(\\sum\\limits_{j=1}^m\\left(\\sum_{i=1}^n a_{i,j}^p\\right)^{q/p}\\right)^{1/q}.\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
