{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJV_pZtVtfE"
      },
      "source": [
        "# Tutorial 2\n",
        "\n",
        "With this tutorial, we will jump into the `pytorch` Autograd functionality and further implement the Softmax Regression.\n",
        "\n",
        "## 1. Autograd\n",
        "\n",
        "Let\n",
        "\n",
        "$$\\begin{align}\n",
        "f:\\mathbb{R} &\\to\\mathbb{R}\\\\\n",
        "x&\\mapsto 0.7x^4+4\\sin(x)-0.5x^3\n",
        "\\end{align}.$$\n",
        "\n",
        "a) Plot $f(x)$ for $x \\in [2.5,2.5]$.\n",
        "\n",
        "b) Use the `pytorch` package to calculate the gradient of $f$ in $x = 2.5$. (Hint: `.backward()`)\n",
        "\n",
        "c) Use gradient descent (with a stepsize of $0.1$ and $20$ steps) to find the minimum.\n",
        "\n",
        "d) Modify your code of c) to log the values after each step and plot points for each step of gradient descent (Hint: `pyplot.scatter`, see [documentation](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPh-pw49Yz6V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a58sWI-hVtfH"
      },
      "source": [
        "## 2. Softmax Regression\n",
        "\n",
        "### Dataset\n",
        "\n",
        "At first load the [breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset) from [Scikit-learn](https://scikit-learn.org/stable/index.html) and split it into training and testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVzMFNorVtfI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMW83BrVVtfR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "features, labels = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train= torch.tensor(X_train, dtype = torch.float)\n",
        "Y_train= torch.tensor(Y_train, dtype = torch.long)\n",
        "X_test= torch.tensor(X_test, dtype = torch.float)\n",
        "Y_test= torch.tensor(Y_test, dtype = torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9sAeUcKVtfW"
      },
      "source": [
        "Take a look at the [documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset) of the dataset. How many observations and features are included? What are the features? How many different classes exist? etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2eb0PCfVtfX"
      },
      "source": [
        "* How is the benign class encoded?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F314-a-PVtfZ"
      },
      "source": [
        "### Softmax Regression from Scratch\n",
        "\n",
        "In a classification problem with $K$ classes, we can represent the outcome space as $\\mathcal{Y}:=\\{1,\\dots,K\\}$ (or $\\{0,\\dots,K-1\\}$).<br>\n",
        "The softmax regression model assumes that the conditional probablilty\n",
        "$$ P(Y=j|X=x)= \\frac{\\exp(w_j^TX+b_j)}{\\sum_\\limits{k=1}^{K}\\exp(w_k^TX+b_k)}$$\n",
        "for all $j=\\{1,\\dots,K\\}$. This is a combination of $K$ linear functions and the softmax function (normalized exponential function)\n",
        "$$\\begin{align}\n",
        "\\text{softmax}:\\mathbb{R}^K&\\to [0,1]^K\\\\\n",
        "\\begin{pmatrix}x_1\\\\ \\vdots \\\\ x_K\\end{pmatrix}&\\mapsto\\left(\\frac{\\exp(x_1)}{\\sum_\\limits{k=1}^{K}\\exp(x_k)},\\dots,\\frac{\\exp(x_K)}{\\sum_\\limits{k=1}^{K}\\exp(x_k)}\\right)^T.\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_O7r_6wVtfa"
      },
      "source": [
        "* First, start by initializing the weights as in the lecture (use the seed $42$). How many weights do we need?\n",
        "* Next, define a sofmax function, which takes in a $m\\times p$ dimensional tensor and applies the softmax function for each row (the broadcasting mechanism can simplify your solutions). The result should be an $m\\times p$ dimensional tensor of probabilities, which sum up to $1$.\n",
        "* Use the weights and the softmax-function to define a softmax regression function, which returns the estimated conditional probablities for each class. Try the model on the first three observations of your training set.\n",
        "* What would your final prediction look like, according to these probabilities (for implementation `torch.max`, see [documentation](https://pytorch.org/docs/stable/generated/torch.max.html), might be helpful)? How do the true labels look like (to compare both use `torch.eq`)?\n",
        "* Evaluate the random model on your training and test set by evaluating the accuarcy (share of correct predictions).\n",
        "\n",
        "For training, we will need another loss function (why?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMntbtZ5Vtfc"
      },
      "source": [
        "### Cross-Entropy\n",
        "\n",
        "At first try to cross entropy loss\n",
        "\n",
        "$$\\begin{align}\n",
        "l:&\\mathcal{Y}_K\\times[0,1]^K\\to \\mathbb{R}^+ \\\\\n",
        "&(y_1,y_2)\\mapsto -\\sum_{j=1}^K y_{1,j} \\log(y_{2,j}). \\end{align}$$\n",
        "\n",
        "Remark that the outcome is not encoded in one-hot-encoding (as in the formula). This might be helpful to implement the loss concisely. Hint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQpQ3eZ1Vtfd",
        "outputId": "92b200af-8de0-41ca-962f-bb562bb72953"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3000, 0.5000, 0.3000])"
            ]
          },
          "execution_count": 112,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = torch.tensor([0, 1, 0]) # three samples with corresponding classes 0,1 and 0\n",
        "y_hat = torch.tensor([[0.3, 0.7], [0.5, 0.5],[0.8, 0.2]])# three predictions with corresponding probablities\n",
        "y_hat[[0, 1, 0], y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjUp3VgoVtfk"
      },
      "source": [
        "* Next, try to calculate the average loss on the training sample.\n",
        "* Calculate the gradient to obtain the gradient of the weights and update the weights with learning rate $0.01$. Here we dont need to use stochastic gradient descent, since the sample is quite small.\n",
        "* Try to predict new probabilities $\\hat{y}$ for the training observations. What does go wrong? (depends on the initialization).\n",
        "* What happens if you combine the softmax function and the cross entropy function (try to calculate the solution analytically, assuming that the true outcome is the $j$-th class)?<br> Use the the [\"LogSumExp trick\"](https://en.wikipedia.org/wiki/LogSumExp): $\\log\\left(\\sum_j \\exp(x_j)\\right)\\approx\\max_j x_j$.\n",
        "* Now try to reinitialize the weights and use the results from the previous exercise to train your regression model. (Define a function which specifies $loss(x,y)= \\max_k x_k-x_j$).\n",
        "* Next, evaluate the accuracy on your trained model. Do you really need to apply the `softmax` function to determine which is the most probable class?\n",
        "* Is it always a good idea to return the most likely label? Especially in this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDstLZ4rVtfl"
      },
      "source": [
        "### Concise Implementation of Softmax Regression\n",
        "\n",
        "In this exercise, we will implement the training using SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyM77a7wVtfm"
      },
      "source": [
        "* Next, construct a `DataLoader` (to apply SGD) for the training set by using `torch.utils.data.DataLoader` and set a seed for the random number generator. Additionally, create an `DataLoader` for the test set.\n",
        "* Read through the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) of `nn.CrossEntropyLoss` to create the loss. Furhter use `nn.Sequential` and `nn.Linear` to create the model.\n",
        "* Initialize the weights of your model and try to evaluate the accuracy using the `DataLoader`.\n",
        "* Now train your model and save the loss and accuracy in each epoch (on the training and on the test set). Here, `reduction = 'sum' ` might be helpful for the loss.\n",
        "* Try to create two plots displaying the accuracy and the losses."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}