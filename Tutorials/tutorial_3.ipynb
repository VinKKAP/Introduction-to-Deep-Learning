{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylase9l7HY-0"
      },
      "source": [
        "# Tutorial 3\n",
        "\n",
        "## 1. Backpropagation (without Python)\n",
        "\n",
        "Take a look at the following simple neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kxObIJGHY-7"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/SvenKlaassen/DL-Lecture-Figures/main/figures/simple_nn.png\" alt=\"Simple Network\" style=\"width: 800px;\"/><br>\n",
        "<b>Figure 1:</b> Simple Neural Network.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OHqvm9kHY-8"
      },
      "source": [
        "Here, $\\sigma_1$ and $\\sigma_2$ are ReLU. Further the weights are set to\n",
        "\n",
        "$$ v_1=\\begin{pmatrix} 1\\\\2 \\end{pmatrix}, v_2=\\begin{pmatrix} -3\\\\4 \\end{pmatrix} \\text{ and }w=\\begin{pmatrix} -2\\\\1\\\\0.5 \\end{pmatrix}.$$\n",
        "\n",
        "Assume you observe the features $x=(1,1)^T$ with outcome $y=4$.\n",
        "\n",
        "* Calculate one forward pass through the model to obtain the predicted value $\\hat{y}$. What is the corresponding squared loss?\n",
        "\n",
        "* Perform a backward pass through the model and calculate the gradient of the loss function with respect to all parameters of the model.\n",
        "\n",
        "* (Optional) Validate your results using python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rDCiIVbHY--"
      },
      "source": [
        "## 2. Neural Network\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Start by importing the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ICpRyCHY-_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUM_LJDYHY_B"
      },
      "source": [
        "Next, we load the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVwbIrf7HY_C"
      },
      "outputs": [],
      "source": [
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True,   transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True,   transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])) #normalize on the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8C1rwK0HY_E"
      },
      "source": [
        "The dataset contains $70,000$ observations of handwritten digits with corresponding labels. Here, the data is transformed to a tensor and normalized ($0.1307$ and $0.3081$ are the mean and the standard deviation on the training set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OIxVzQDHY_F"
      },
      "outputs": [],
      "source": [
        "len(mnist_train), len(mnist_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBvhVp-wHY_G"
      },
      "source": [
        "* Construct a `DataLoader` for the training set by using `torch.utils.data.DataLoader` directly (batch sizes of $64$ and $128$) and set a seed for the random number generator. Additionally, create an `DataLoader` for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYGAX3TDHY_H"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(mnist_train,10,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFuVMMgPHY_H"
      },
      "source": [
        "* Take a look at the data examples by iterating the `DataLoader` once over the training set. Save the batch (e.g. as `example`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzhlOVlAHY_I"
      },
      "source": [
        "**Digression:** Another helpful built-in function of Python is `enumerate`. Try looping over the following list using `enumerate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e42vC0EHY_J"
      },
      "outputs": [],
      "source": [
        "list = ['Item 1', 'Item 2', 'Item 3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt0sQinvHY_J"
      },
      "outputs": [],
      "source": [
        "for counter, item in enumerate(list):\n",
        "    print(counter, item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5HLRsbDHY_K"
      },
      "source": [
        "For our purposes, `enumerate` can be used on the `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0tjEiP0HY_K"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_features, example_labels) = next(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5-xP4dMHY_K"
      },
      "source": [
        "The following code chunk displays the features as an imange and the corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdg2nEKCHY_L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(4):\n",
        "    plt.subplot(1,4,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_features[i][0], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Label: {}\".format(example_labels[i]))\n",
        "    plt.xticks([0,14,28])\n",
        "    plt.yticks([0,14,28])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHnl6x9IHY_M"
      },
      "source": [
        "## Implementing a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TnWZzLyHY_M"
      },
      "source": [
        "Import the `torch.nn` and construct a sequential neural network (choose your own structure; maybe not too large) for classification. Starting with the `nn.Flatten` layer migth be very helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ4jNeNgHY_N"
      },
      "source": [
        "* Use the `CrossEntropyLoss` (`reduction='sum'` might be helpful later) and `torch.optim.Adam`for optimization.\n",
        "* Initialize all weights of your network from a normal distribution with standard deviation $0.01$.\n",
        "* Before we start training our network, use our `train_loader` and `test_loader` to evaluate the loss on our training and testing data (this might take a while).\n",
        "* Specify the number of epochs to $1$ and start training your network. Afterwards evaluate the loss.\n",
        "* We would like to get more information printed during the training process. Increase the number of epochs to $5$ and every $50$ batches print the current epoch and loss on the batch (you can add the number of used observations in the epoch as well). Further add the end of each epoch evaluate the loss on the training set. Before starting reinitialize the weights randomly.\n",
        "* Next, take a look at a specific predicition on your example batch from above and compare the predictions to the corresponding labels.\n",
        "* Use this to evaluate the share of accuracy (share correctly predicted labels) on the test set at each epoch. Additionally, log all the printed losses. Again, before starting reinitialize the weights randomly.\n",
        "* Plot the logged losses in a suitable plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5pNvS56HY_N"
      },
      "source": [
        "Neural network modules as well as optimizers have the ability to save and load their internal state using `.state_dict()`.\n",
        "`load_state_dict(state_dict)`. See [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for saving and loading models. What does the dictionary save?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywxRUltiHY_O"
      },
      "source": [
        "### GPUs\n",
        "Now, use GPUs for training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBc72qBbHY_O"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}